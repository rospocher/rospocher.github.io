@article{2026inf,
 abstract = {Automata and artificial intelligence (AI) have long occupied a central place in cultural and artistic imagination, and the recent proliferation of AI-generated artworks has intensified debates about authorship, creativity, and human agency. Empirical studies show that audiences often perceive AI-generated works as less authentic or emotionally resonant than human creations, with authorship attribution strongly shaping esthetic judgments. Yet little attention has been paid to how AI systems themselves evaluate creative authorship. This study investigates how large language models (LLMs) evaluate literary quality under different framings of authorship---Human, AI, or Human+AI collaboration. Using a questionnaire-based experimental design, we prompted four instruction-tuned LLMs (ChatGPT 4, Gemini 2, Gemma 3, and LLaMA 3) to read and assess three short stories in Italian, originally generated by ChatGPT 4 in the narrative style of Roald Dahl. For each story × authorship condition × model combination, we collected 100 questionnaire completions, yielding 3600 responses in total. Across esthetic, literary, and inclusiveness dimensions, the stated authorship systematically conditioned model judgments: identical stories were consistently rated more favorably when framed as human-authored or human--AI co-authored than when labeled as AI-authored, revealing a robust negative bias toward AI authorship. Model-specific analyses further indicate distinctive evaluative profiles and inclusiveness thresholds across proprietary and open-source systems. Our findings extend research on attribution bias into the computational realm, showing that LLM-based evaluations reproduce human-like assumptions about creative agency and literary value. We publicly release all materials to facilitate transparency and future comparative work on AI-mediated literary evaluation.},
 article-number = {95},
 author = {Marco Rospocher and Massimo Salgaro and Simone Rebora},
 bdsk-url-1 = {https://www.mdpi.com/2078-2489/17/1/95},
 bdsk-url-2 = {https://doi.org/10.3390/info17010095},
 date-added = {2026-01-17 09:56:29 +0100},
 date-modified = {2026-01-17 09:56:58 +0100},
 doi = {10.3390/info17010095},
 issn = {2078-2489},
 journal = {Information},
 number = {1},
 title = {Machines Prefer Humans as Literary Authors: Evaluating Authorship Bias in Large Language Models},
 volume = {17},
 year = {2026}
}
